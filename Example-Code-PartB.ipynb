{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Code Example - Part B\n",
    "\n",
    "This achieves an accuracy of 93.26% on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide some basic operators like matrix multiplication\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class\n",
    "class Module:\n",
    "    @property\n",
    "    def params(self): # trainable parameters\n",
    "        return []\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward( *args, **kwargs)\n",
    "\n",
    "# sequential module\n",
    "class Sequential(Module, list):\n",
    "    def __init__(self, *module_lst):\n",
    "        super().__init__(module_lst)\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return sum([m.params for m in self], []) # concat all params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for module in self:\n",
    "            y = module(y)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dx = dy\n",
    "        for module in self[::-1]:\n",
    "            dx = module.backward(dx)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "\n",
    "This implements the [Softplus](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html) function.\n",
    "\n",
    "$y = \\frac{1}{\\beta} \\ln(1+e^{\\beta x})$\n",
    "\n",
    "$y' = \\frac{1}{1+e^{-\\beta x}}$\n",
    "\n",
    "Default: $\\beta=1$\n",
    "\n",
    "$e^{\\beta x}$ might be too large and unstable; so we use linear function to approximate it when $\\beta x$ is above the threshold $20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softplus(Module):\n",
    "    def __init__(self, beta=1.0, threshold=20.0):\n",
    "        assert beta > 0 and threshold > 0\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.beta_x = self.beta * x # save the input for backward use\n",
    "        y = np.log(1 + np.exp(self.beta_x)) / self.beta\n",
    "        y_relu = np.where(x > 0, x, 0)\n",
    "        return np.where(x < self.threshold, y, y_relu)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        grad = 1 / (1 + np.exp(-self.beta_x))\n",
    "        grad_relu = np.where(self.beta_x > 0, 1, 0)\n",
    "        return dy * np.where(self.beta_x < self.threshold, grad, grad_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearNoBias\n",
    "\n",
    "This implements the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer but without the bias term.\n",
    "\n",
    "$y = x W^T$\n",
    "\n",
    "$dy/dx = W$\n",
    "\n",
    "$dy/dW = x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoBias(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weight = (np.random.rand(out_features, in_features) * 2 - 1) / in_features ** 0.5\n",
    "        self.weight_grad = np.zeros_like(self.weight)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [dict(val=self.weight, grad=self.weight_grad)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight.T\n",
    "\n",
    "    def backward(self, dy):\n",
    "        self.weight_grad[:] = dy.T @ self.x\n",
    "        return dy @ self.weight\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n",
    "\n",
    "This implements the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x, num_classes=10):\n",
    "    y = np.zeros([len(x), num_classes])\n",
    "    y[np.arange(len(x)), x] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Module):    \n",
    "    def forward(self, x_logit, x_target):\n",
    "        self.x_logit = x_logit\n",
    "        self.x_target = x_target\n",
    "        \n",
    "        # softmax\n",
    "        x_logit_sub = np.exp(x_logit - np.max(x_logit, axis=1, keepdims=True))\n",
    "        x_softmax = x_logit_sub / np.sum(x_logit_sub, axis=1, keepdims=True)\n",
    "        x_softmax = np.clip(x_softmax, min=1e-15) # to avoid zero values\n",
    "        self.x_softmax = x_softmax # save for backward\n",
    "\n",
    "        # loss of each item\n",
    "        loss_x = -np.log(x_softmax)[np.arange(len(x_target)), x_target]\n",
    "\n",
    "        # average\n",
    "        return loss_x.mean()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * (self.x_softmax - onehot(self.x_target)) / len(self.x_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optim:\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for idx in range(len(self.params)):\n",
    "            self.params[idx][\"grad\"][:] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optim):\n",
    "    def step(self):\n",
    "        for idx in range(len(self.params)):\n",
    "            self.params[idx][\"val\"] -= self.lr * self.params[idx][\"grad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential(\n",
    "    LinearNoBias(784, 512), Softplus(),\n",
    "    LinearNoBias(512, 256), Softplus(),\n",
    "    LinearNoBias(256, 128), Softplus(),\n",
    "    LinearNoBias(128, 10),\n",
    ")\n",
    "\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch and torchvision provide some very handy utilities for dataset loading\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.transforms as tv_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "num_epochs = 32\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.MNIST(\n",
    "        root=\"./data\", train=is_train, download=True, \n",
    "        transform=tv_transforms.Compose([ # preprocessing pipeline for input images\n",
    "            tv_transforms.ToTensor(),\n",
    "            tv_transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]))\n",
    "    loader[data_type] = DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch=  1, iter=  100] loss: 2.287\n",
      "[epoch=  1, iter=  200] loss: 2.224\n",
      "[epoch=  1, iter=  300] loss: 1.940\n",
      "[epoch=  1, iter=  400] loss: 1.274\n",
      "[epoch=  2, iter=  100] loss: 0.770\n",
      "[epoch=  2, iter=  200] loss: 0.669\n",
      "[epoch=  2, iter=  300] loss: 0.596\n",
      "[epoch=  2, iter=  400] loss: 0.553\n",
      "[epoch=  3, iter=  100] loss: 0.509\n",
      "[epoch=  3, iter=  200] loss: 0.477\n",
      "[epoch=  3, iter=  300] loss: 0.451\n",
      "[epoch=  3, iter=  400] loss: 0.442\n",
      "[epoch=  4, iter=  100] loss: 0.431\n",
      "[epoch=  4, iter=  200] loss: 0.427\n",
      "[epoch=  4, iter=  300] loss: 0.414\n",
      "[epoch=  4, iter=  400] loss: 0.406\n",
      "[epoch=  5, iter=  100] loss: 0.398\n",
      "[epoch=  5, iter=  200] loss: 0.389\n",
      "[epoch=  5, iter=  300] loss: 0.380\n",
      "[epoch=  5, iter=  400] loss: 0.391\n",
      "[epoch=  6, iter=  100] loss: 0.374\n",
      "[epoch=  6, iter=  200] loss: 0.365\n",
      "[epoch=  6, iter=  300] loss: 0.385\n",
      "[epoch=  6, iter=  400] loss: 0.358\n",
      "[epoch=  7, iter=  100] loss: 0.364\n",
      "[epoch=  7, iter=  200] loss: 0.352\n",
      "[epoch=  7, iter=  300] loss: 0.349\n",
      "[epoch=  7, iter=  400] loss: 0.360\n",
      "[epoch=  8, iter=  100] loss: 0.356\n",
      "[epoch=  8, iter=  200] loss: 0.340\n",
      "[epoch=  8, iter=  300] loss: 0.346\n",
      "[epoch=  8, iter=  400] loss: 0.346\n",
      "[epoch=  9, iter=  100] loss: 0.331\n",
      "[epoch=  9, iter=  200] loss: 0.329\n",
      "[epoch=  9, iter=  300] loss: 0.341\n",
      "[epoch=  9, iter=  400] loss: 0.339\n",
      "[epoch= 10, iter=  100] loss: 0.315\n",
      "[epoch= 10, iter=  200] loss: 0.333\n",
      "[epoch= 10, iter=  300] loss: 0.335\n",
      "[epoch= 10, iter=  400] loss: 0.327\n",
      "[epoch= 11, iter=  100] loss: 0.336\n",
      "[epoch= 11, iter=  200] loss: 0.322\n",
      "[epoch= 11, iter=  300] loss: 0.315\n",
      "[epoch= 11, iter=  400] loss: 0.319\n",
      "[epoch= 12, iter=  100] loss: 0.319\n",
      "[epoch= 12, iter=  200] loss: 0.311\n",
      "[epoch= 12, iter=  300] loss: 0.326\n",
      "[epoch= 12, iter=  400] loss: 0.305\n",
      "[epoch= 13, iter=  100] loss: 0.313\n",
      "[epoch= 13, iter=  200] loss: 0.320\n",
      "[epoch= 13, iter=  300] loss: 0.300\n",
      "[epoch= 13, iter=  400] loss: 0.309\n",
      "[epoch= 14, iter=  100] loss: 0.305\n",
      "[epoch= 14, iter=  200] loss: 0.298\n",
      "[epoch= 14, iter=  300] loss: 0.313\n",
      "[epoch= 14, iter=  400] loss: 0.299\n",
      "[epoch= 15, iter=  100] loss: 0.301\n",
      "[epoch= 15, iter=  200] loss: 0.299\n",
      "[epoch= 15, iter=  300] loss: 0.293\n",
      "[epoch= 15, iter=  400] loss: 0.298\n",
      "[epoch= 16, iter=  100] loss: 0.290\n",
      "[epoch= 16, iter=  200] loss: 0.292\n",
      "[epoch= 16, iter=  300] loss: 0.302\n",
      "[epoch= 16, iter=  400] loss: 0.289\n",
      "[epoch= 17, iter=  100] loss: 0.283\n",
      "[epoch= 17, iter=  200] loss: 0.283\n",
      "[epoch= 17, iter=  300] loss: 0.299\n",
      "[epoch= 17, iter=  400] loss: 0.290\n",
      "[epoch= 18, iter=  100] loss: 0.282\n",
      "[epoch= 18, iter=  200] loss: 0.288\n",
      "[epoch= 18, iter=  300] loss: 0.292\n",
      "[epoch= 18, iter=  400] loss: 0.290\n",
      "[epoch= 19, iter=  100] loss: 0.281\n",
      "[epoch= 19, iter=  200] loss: 0.285\n",
      "[epoch= 19, iter=  300] loss: 0.281\n",
      "[epoch= 19, iter=  400] loss: 0.287\n",
      "[epoch= 20, iter=  100] loss: 0.271\n",
      "[epoch= 20, iter=  200] loss: 0.288\n",
      "[epoch= 20, iter=  300] loss: 0.283\n",
      "[epoch= 20, iter=  400] loss: 0.270\n",
      "[epoch= 21, iter=  100] loss: 0.278\n",
      "[epoch= 21, iter=  200] loss: 0.286\n",
      "[epoch= 21, iter=  300] loss: 0.275\n",
      "[epoch= 21, iter=  400] loss: 0.271\n",
      "[epoch= 22, iter=  100] loss: 0.275\n",
      "[epoch= 22, iter=  200] loss: 0.283\n",
      "[epoch= 22, iter=  300] loss: 0.265\n",
      "[epoch= 22, iter=  400] loss: 0.268\n",
      "[epoch= 23, iter=  100] loss: 0.282\n",
      "[epoch= 23, iter=  200] loss: 0.270\n",
      "[epoch= 23, iter=  300] loss: 0.262\n",
      "[epoch= 23, iter=  400] loss: 0.265\n",
      "[epoch= 24, iter=  100] loss: 0.262\n",
      "[epoch= 24, iter=  200] loss: 0.267\n",
      "[epoch= 24, iter=  300] loss: 0.280\n",
      "[epoch= 24, iter=  400] loss: 0.250\n",
      "[epoch= 25, iter=  100] loss: 0.263\n",
      "[epoch= 25, iter=  200] loss: 0.264\n",
      "[epoch= 25, iter=  300] loss: 0.259\n",
      "[epoch= 25, iter=  400] loss: 0.264\n",
      "[epoch= 26, iter=  100] loss: 0.265\n",
      "[epoch= 26, iter=  200] loss: 0.261\n",
      "[epoch= 26, iter=  300] loss: 0.255\n",
      "[epoch= 26, iter=  400] loss: 0.265\n",
      "[epoch= 27, iter=  100] loss: 0.273\n",
      "[epoch= 27, iter=  200] loss: 0.248\n",
      "[epoch= 27, iter=  300] loss: 0.242\n",
      "[epoch= 27, iter=  400] loss: 0.254\n",
      "[epoch= 28, iter=  100] loss: 0.251\n",
      "[epoch= 28, iter=  200] loss: 0.248\n",
      "[epoch= 28, iter=  300] loss: 0.247\n",
      "[epoch= 28, iter=  400] loss: 0.267\n",
      "[epoch= 29, iter=  100] loss: 0.251\n",
      "[epoch= 29, iter=  200] loss: 0.259\n",
      "[epoch= 29, iter=  300] loss: 0.241\n",
      "[epoch= 29, iter=  400] loss: 0.252\n",
      "[epoch= 30, iter=  100] loss: 0.241\n",
      "[epoch= 30, iter=  200] loss: 0.252\n",
      "[epoch= 30, iter=  300] loss: 0.249\n",
      "[epoch= 30, iter=  400] loss: 0.242\n",
      "[epoch= 31, iter=  100] loss: 0.242\n",
      "[epoch= 31, iter=  200] loss: 0.252\n",
      "[epoch= 31, iter=  300] loss: 0.242\n",
      "[epoch= 31, iter=  400] loss: 0.238\n",
      "[epoch= 32, iter=  100] loss: 0.237\n",
      "[epoch= 32, iter=  200] loss: 0.252\n",
      "[epoch= 32, iter=  300] loss: 0.229\n",
      "[epoch= 32, iter=  400] loss: 0.251\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(net.params)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.numpy(), target.numpy()\n",
    "        img = img.reshape(-1, 784)\n",
    "        \n",
    "        loss = loss_fn(net(img), target)\n",
    "\n",
    "        net.backward(loss_fn.backward(loss))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 93.26%\n"
     ]
    }
   ],
   "source": [
    "# for each test image\n",
    "correct, total = 0, 0\n",
    "for img, target in loader[\"test\"]:\n",
    "    img, target = img.numpy(), target.numpy()\n",
    "    img = img.reshape(-1, 784)\n",
    "    \n",
    "    # make prediction\n",
    "    pred = net(img)\n",
    "    \n",
    "    # accumulate\n",
    "    total += len(target)\n",
    "    correct += (np.argmax(pred, axis=1) == target).sum()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

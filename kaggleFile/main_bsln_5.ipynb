{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchvision.datasets as tv_datasets\n","import torchvision.transforms as tv_transforms\n","import numpy as np\n","from imp import reload\n","\n","\n","# ========= hyper parameters =========\n","num_epochs = 256\n","batch_size = 32\n","num_workers = 2\n","\n","# choose path, dataset, model\n","run_on_local = 0 # 1 for True, 0 for False\n","dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","model_id = -2 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","optimizer_id = 0 # 0 for Adam, 1 for SGD\n","data_aug = True\n","\n","# ===================================\n","\n","\n","\n","# add path for dataset/model script depending on testing on local or running in kaggle:\n","import sys\n","\n","# some experimental setup\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device(type='cpu')\n","kaggle = \"/kaggle/input/cifar-10-dlhw1-2/data\"\n","local = \"../data\"\n","\n","if run_on_local:\n","    data_path = local\n","    sys.path.append('./code')\n","    \n","else:\n","    data_path = kaggle\n","    sys.path.append('/kaggle/input')\n","\n","from mymodels import models\n","print('successfully load all pac')\n"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["\n","# huge main function for tuning params, simple logic: \n","\n","# choose optimizer -> choose dataloading strategy -> prepare data -> initialize model -> training loop -> print training info -> visualize training resilts.\n","\n","def main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers, dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug, modelnames = modelnames):\n","\n","    # choose optimizer, both options use poprlar parameters\n","    optim_name = [\"Adam\", \"SGD\"]\n","    optim_kwargs = {\"Adam\": dict(lr=3e-4, weight_decay=1e-6,),\n","        \"SGD\": dict(lr = 1e-3, momentum = 0.9)}\n","\n","    # choose whether or not to use data augmentation strategy\n","    if data_aug:\n","        # ============ MNIST transform with aug ===========\n","        if dataset1 == 'MNIST':\n","            transformation = dict()\n","            for data_type in (\"train\", \"test\"):\n","                is_train = data_type==\"train\"\n","                transformation[data_type] = tv_transforms.Compose(([\n","                tv_transforms.RandomRotation(15),  \n","                tv_transforms.RandomHorizontalFlip(),\n","                # tv_transforms.RandomAffine(0, translate=(0.1, 0.1)),  \n","                # tv_transforms.RandomResizedCrop(28, scale=(0.9, 1.1)), \n","                tv_transforms.ToTensor(),  \n","                tv_transforms.Normalize((0.1307,), (0.3081,))  \n","            ] if is_train else [\n","                tv_transforms.ToTensor(),\n","                tv_transforms.Normalize((0.1307,), (0.3081,))]))\n","\n","        elif dataset1 == 'CIFAR10':\n","        # ============== cifar transform with aug ================\n","            transformation = dict()\n","            for data_type in (\"train\", \"test\"):\n","                is_train = data_type==\"train\"\n","                transformation[data_type] = tv_transforms.Compose((\n","                    [\n","                        \n","                        tv_transforms.RandomRotation(degrees=15),\n","                        tv_transforms.RandomHorizontalFlip(),\n","                        tv_transforms.RandomAffine(degrees = 0, translate = (0.1, 0.1)),\n","                        tv_transforms.ColorJitter(\n","                            brightness=0.2, \n","                            contrast=0.2,\n","                            saturation=0.2,\n","                            hue=0.1\n","                        ),\n","                        \n","\n","                        tv_transforms.ToTensor(),\n","                        tv_transforms.Normalize(\n","                            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n","                        )\n","                    ] if is_train else [\n","                        tv_transforms.ToTensor(),\n","                        tv_transforms.Normalize(\n","                            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n","                        )\n","                    ]\n","                ))\n","\n","    # ============= transformation without data augmentation ==================\n","    else:\n","        if dataset1 == 'CIFAR10':\n","            transformation = dict()\n","            for data_type in ('train', 'test'):\n","                is_train = data_type == 'train'\n","                transformation[data_type] = tv_transforms.Compose(([\n","                tv_transforms.RandomRotation(degrees=15),\n","                tv_transforms.RandomHorizontalFlip(),\n","                tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n","                tv_transforms.ToTensor(),\n","                tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","            ] if is_train else  [   \n","                tv_transforms.ToTensor(),\n","                tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","            ]))\n","        elif dataset1 == 'MNIST':\n","            transformation = dict()\n","            for data_type in ('train', 'test'):\n","                is_train = data_type == 'train'\n","                transformation[data_type] = tv_transforms.Compose(([\n","                tv_transforms.ToTensor(),  \n","                tv_transforms.Normalize((0.1307,), (0.3081,))  \n","            ] if is_train else  [   \n","                tv_transforms.ToTensor(),  \n","                tv_transforms.Normalize((0.1307,), (0.3081,))  \n","            ]))\n","\n","    # ======= prepare datasets ========\n","    dataloader = getattr(tv_datasets, dataset1)\n","    dataset, loader = {}, {}\n","    for data_type in (\"train\", \"test\"):\n","        is_train = data_type==\"train\"\n","        # root=./data: create data file in root path if there is none, dataset size ~340MB\n","        # path kaggle: \"/kaggle/input/cifar-10-dlhw1/data\"\n","        # path local: \"../data\"\n","        dataset[data_type] = dataloader(\n","            root=data_path, train=is_train, download=True, transform=transformation[data_type],\n","        )\n","        loader[data_type] = torch.utils.data.DataLoader(\n","            dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers\n","        )\n","                        \n","    # ======== training loop =========\n","\n","    # always re-import models when changing settings: avoid using weights from before.\n","\n","    print('successfully load all pac')\n","    net = getattr(models, modelnames[model_id])\n","    # net(dummy_input)\n","\n","    # move to device\n","    net.to(device)\n","\n","    # print the info of hyper/parameters\n","    # avoid checking parameter info before passing data into the model if you use LazyConv2d next time, otherwise you need to extract 1 entry of data for initializing model\n","\n","    # print(f'hyperparams: num_epochs: {num_epochs}, batch_size: {batch_size}, num_workers: {num_workers}, run_on_local: {run_on_local}, dataset: {dataset1}, model_id: {model_id}-{modelnames[model_id]}, optimizer_id: {optimizer_id}-{optim_name[optimizer_id]}, data_aug: {int(data_aug)}')\n","    # print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n","\n","    # the network optimizer\n","    optimizer = getattr(optim, optim_name[optimizer_id])(net.parameters(), **optim_kwargs[optim_name[optimizer_id]])\n","\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # storing statistics for plotting/recording:\n","    \n","    loss_train = []\n","    loss_test = []\n","    accuracy_test = []\n","\n","    # training loop\n","    net.train()\n","    for epoch in range(num_epochs):\n","\n","        running_loss = 0.0\n","        for i, (img, target) in enumerate(loader[\"train\"]):\n","            img, target = img.to(device), target.to(device)\n","\n","            pred = net(img)\n","            loss = criterion(pred, target)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()/batch_size\n","\n","        # eval at each epoch: computation-consuming? i just don't care\n","        testing_loss, correct, total =0.0, 0, 0\n","        with torch.no_grad():\n","            for img, target in loader[\"test\"]:\n","                img, target = img.to(device), target.to(device)\n","                \n","                # make prediction\n","                pred = net(img)\n","                loss = criterion(pred, target)\n","                testing_loss += loss.item()/batch_size\n","                \n","                # accumulate\n","                total += len(target)\n","                correct += (torch.argmax(pred, dim=1) == target).sum().item()\n","                \n","        loss_train.append(running_loss)\n","        loss_test.append(testing_loss)\n","        accuracy_test.append(correct / total)\n","\n","        print(f\"Epchs: {epoch+1}, train loss: {loss_train[-1]:.2f}, test loss: {loss_test[-1]:.2f}, test Accuracy: {100 * accuracy_test[-1]:.2f}%\")\n","\n","    print(\"Finished Training\")\n","\n","    # ===== save trained weights =====\n","    save_name = f'{modelnames[model_id]}_{optim_name[optimizer_id]}_{int(data_aug)}aug'\n","    PATH = '/kaggle/working/'+ 'weights_' + save_name +'.pth'\n","    torch.save(net.state_dict(), PATH)\n","\n","    # save accuracy/loss statistics\n","    np.save('stats_' + save_name+'_accuracy_test.npy', np.array(accuracy_test))\n","    np.save('stats_' + save_name+'_loss_train.npy', np.array(loss_train))\n","    np.save('stats_' + save_name+'_loss_test.npy', np.array(loss_test))\n","    # reload\n","    # loaded_data = np.load('data.npy').tolist()\n","\n","\n","    # plotting training_loss and testing_accuracy curve:\n","    import matplotlib.pyplot as plt\n","\n","    def visualize_accuracy(accuracy_test, loss_test, loss_train, save_path):\n","        import matplotlib.gridspec as gridspec\n","\n","        plt.figure(figsize=(10, 10))  \n","        gs = gridspec.GridSpec(2, 2,  \n","                            height_ratios=[5, 5], \n","                            width_ratios=[5, 5],   \n","                            hspace=0.2, wspace=0.2)  \n","\n","        \n","        ax1 = plt.subplot(gs[0, 0])  \n","        ax2 = plt.subplot(gs[0, 1])  \n","        ax3 = plt.subplot(gs[1, :])  \n","\n","        \n","        ax1.plot(loss_train, marker='o', linestyle='-', color='g', label='Train Loss')\n","        ax1.set_xlabel('Epochs')\n","        ax1.set_ylabel('Loss Value')\n","        ax1.set_title('Training Loss Progression')\n","        ax1.legend()\n","        ax1.grid(True)\n","        ax1.set_xlim(0, len(loss_train))  \n","        ax1.set_ylim(min(loss_train)*0.9, max(loss_train)*1.1)  \n","\n","        \n","        ax2.plot(loss_test, marker='o', linestyle='-', color='r', label='Test Loss')\n","        ax2.set_xlabel('Epochs')\n","        ax2.set_ylabel('Loss Value')\n","        ax2.set_title('Validation Loss Progression')\n","        ax2.legend()\n","        ax2.grid(True)\n","        ax2.set_xlim(0, len(loss_test))   \n","        ax2.set_ylim(min(loss_test)*0.9, max(loss_test)*1.1)  \n","\n","        \n","        ax3.plot(accuracy_test, marker='o', linestyle='-', color='b', label='Test Accuracy')\n","        ax3.set_xlabel('Epochs')\n","        ax3.set_ylabel('Accuracy (%)')\n","        ax3.set_title('Model Prediction Accuracy')\n","        ax3.legend()\n","        ax3.grid(True)\n","        ax3.set_xlim(0, len(accuracy_test))  \n","        ax3.set_ylim(min(accuracy_test)*0.95, 100 if max(accuracy_test)>95 else max(accuracy_test)*1.05)  \n","\n","        \n","        plt.tight_layout()\n","        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n","        plt.show()\n","\n","    visualize_accuracy(accuracy_test, loss_test, loss_train, 'train_result.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # ======== experiment 1 ===========\n","# # C5L3 base-Adam-dropout all\n","# # ========= hyper parameters =========\n","# num_epochs = 10\n","# batch_size = 64\n","# num_workers = 2\n","\n","# # choose path, dataset, model\n","# run_on_local = 0 # 1 for True, 0 for False\n","# dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","# model_id = -3 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# # 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","# modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","# optimizer_id = 0 # 0 for Adam, 1 for SGD\n","# data_aug = False # False = No extra data aug\n","\n","# # ======= trainer ======\n","# main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers,  dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # ======== experiment 2 ===========\n","# # C5L3 base-SGD-dropout all-no extra aug\n","# # ========= hyper parameters =========\n","# # num_epochs = 150\n","# batch_size = 64\n","# num_workers = 2\n","\n","# # choose path, dataset, model\n","# run_on_local = 0 # 1 for True, 0 for False\n","# dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","# model_id = -3 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# # 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","# modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","# optimizer_id = 1 # 0 for Adam, 1 for SGD\n","# data_aug = True\n","\n","# # ======= trainer ======\n","# main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers,  dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # ======== experiment 3 ===========\n","# # C5L3 base-Adam-dropout linear-BatchNorm\n","# # ========= hyper parameters =========\n","# num_epochs = 150\n","# batch_size = 64\n","# num_workers = 2\n","\n","# # choose path, dataset, model\n","# run_on_local = 0 # 1 for True, 0 for False\n","# dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","# model_id = -2 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# # 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","# modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","# optimizer_id = 0 # 0 for Adam, 1 for SGD\n","# data_aug = True\n","\n","# # ======= trainer ======\n","# main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers,  dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # ======== experiment 4 ===========\n","# # C5L3 base-Adam-dropout none-BatchNorm\n","# # ========= hyper parameters =========\n","# num_epochs = 150\n","# batch_size = 64\n","# num_workers = 2\n","\n","# # choose path, dataset, model\n","# run_on_local = 0 # 1 for True, 0 for False\n","# dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","# model_id = -1 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# # 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","# modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","# optimizer_id = 0 # 0 for Adam, 1 for SGD\n","# data_aug = True\n","\n","# # ======= trainer ======\n","# main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers,  dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ======== experiment 5 ===========\n","# C5L3 base-Adam-dropout linear-BatchNorm\n","# ========= hyper parameters =========\n","num_epochs = 150\n","batch_size = 64\n","num_workers = 2\n","\n","# choose path, dataset, model\n","run_on_local = 0 # 1 for True, 0 for False\n","dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","model_id = -2 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","optimizer_id = 0 # 0 for Adam, 1 for SGD\n","data_aug = False\n","\n","# ======= trainer ======\n","main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers,  dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # ======== experiment 6 ===========\n","# # C5L3 base-Adam-dropout linear-BatchNorm-extra aug-more epo\n","# # ========= hyper parameters =========\n","# num_epochs = 256\n","# batch_size = 64\n","# num_workers = 2\n","\n","# # choose path, dataset, model\n","# run_on_local = 0 # 1 for True, 0 for False\n","# dataset1 = 'CIFAR10' # choose between 'MNIST' or 'CIFAR10'\n","# model_id = -2 # 0/2/4/6/8/10 for MNIST, 1/3/5/7/9/11 for CIFAR10\n","# # 2025-3-8 running: resenet18 and cgg16 on cifar10 v/v4\n","# modelnames = ['C3L2_MNIST', 'C3L2_cifar10', 'C5L3_MNIST', 'C5L3_cifar10', 'ResNet18_MNIST', 'ResNet18_cifar10' ,'ResNet20_MNIST', 'ResNet20_cifar10', 'ResNet50_MNIST', 'ResNet50_cifar10', 'VGG16_MNIST', 'VGG16_cifar10', 'ResNet20_omni', 'C5L4_base_cifar10', 'C5L4_BEST_cifar10', 'C5L4_BEST_cifar10_nodrp']\n","\n","# optimizer_id = 0 # 0 for Adam, 1 for SGD\n","# data_aug = True\n","\n","# # ======= trainer ======\n","# main(num_epochs = num_epochs, batch_size = batch_size, num_workers = num_workers,  dataset1 = dataset1, data_path = data_path, model_id = model_id, optimizer_id = optimizer_id, data_aug = data_aug)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6698758,"sourceId":10794107,"sourceType":"datasetVersion"}],"dockerImageVersionId":30887,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat":4,"nbformat_minor":4}

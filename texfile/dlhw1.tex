\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}

\usepackage[english]{babel}
\usepackage{latexsym,bm,amsmath,amssymb,graphicx,ctex,tikz,systeme,array, soul,scalerel,wrapfig,lipsum}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\usepackage{extarrows}% http://ctan.org/pkg/extarrows
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\eqdef}{\xlongequal{\text{def}}}%
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=10mm,
 right=10mm,
 top=20mm,
 }
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[lighttt]{lmodern}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.2,0.2,0.2}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{keyblue}{rgb}{0,0,0.9}
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codepurple},
    keywordstyle=\color{keyblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=t,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=1pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1
}
% -- Setting up the custom style:
\lstset{style=mystyle}
% ======= code block =======


% ==========================

\lstset{style=mystyle}
\newcommand{\subs}[1]{\subsection*{#1}}
\newcommand{\secs}[1]{\section*{#1}}
 
\usepackage{fancyhdr}
\fancypagestyle{plain}{%  the preset of fancyhdr 
    \fancyhf{} % clear all header and footer fields
    \fancyfoot[R]{Guyuan Xu\\224040074}
    \fancyfoot[L]{\thedate}
    \fancyhead[L]{MDS 5122 Deep Learning and Application}
    % \fancyhead[R]{\theauthor}
}
\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 1em%
  \begin{center}%
  \let \footnote \thanks
    {\LARGE \@title \par}%
    \vskip 1em%
    %{\large \@date}%
  \end{center}%
  \par
  \vskip 1em}
\makeatother

% \usepackage{lipsum}  
% \usepackage{cmbright}



%======================================= document ======================================
%======================================= document ======================================
%======================================= document ======================================

\begin{document}
\title{\raggedright MDS 5122 Assignment 1}
%\author{Guyuan Xu \\224040074}
\date{March 9, 2025}
\maketitle

\noindent\begin{tabular}{@{}ll}
   Guyuan Xu &\href{mailto:224040074@link.cuhk.edu.cn}{224040074} \\
    
%
\end{tabular}

\secs{A. Build a Neural Network Using PyTorch}
\subs{1. reproducing example NeuralNetwork of example code A.}
\textcolor{blue}{Results are summaried in subsection 2.2 together with other settings, codes check notebook}

\subs{2. Factors to improve accuracy}
In my experiment, I tried to use 1) Simple NN structure(which is the baseline model of example code A) and NN with more complex architecture (vgg16 and ResNet18/50); 2) adding BatchNorm layer and dropout layer; 3) different optimziers SGD and Adam with popular \textit{lr} or momentum params settings; 4) with/without data augmentation. \\

We mainly use the model of example code-A as our baseline model to examine how the above factors might affect accuracy. And due to computing resource limitation, i did not conduct strict experiment with strict variable controls. To gain insight is what we want.\\


% \noindent\textbf{- BaseLine Model }: C3L3 (3 Conv layers + 3 Linear layers) on CIFAR-10 dataset\\
% We first start with baseline model in example-code A, the steps of experiment are: 1)compare optimziers with all other settings the same, then pick the best optimziers (in terms of accuracy) for next step; 2) use the best optimizer and data augmentation to see if data augmentation could bring up accuracy in test set; 3) use best optimizer + data augmentation(note that data augmentation might not be useful, shall be given up if so) + \textcolor{red}{BatchNorm Layer} + \textcolor{red}{throw away dropout layer in convolution layer} to see whether we can have better accuracy on test set. 

\noindent\textbf{Experiments Setup}\\

\noindent \textbf{2.1 Baseline Convolutional Neural Network (C5L3) }
\begin{itemize}
  \item   architecture:  5 conv layers, each followed by a dropout layer (rate=0.3); 3 linear layers, each followed by a dropout layer (rate=0.5).
  \item   optimizer: Adam with a learning rate of 3e-4 and weight decay of 1e-6.
  \item   dataset: Trained on the CIFAR-10 dataset. The training set has dimensions (60,000, 3, 32, 32), and the test set has dimensions (10,000, 3, 32, 32).
  \item   Training settings: Trained for 150 epochs with a batch size of 64. Model accuracy was evaluated on the test set.
\end{itemize}
\vspace{5mm}
\noindent \textbf{2.2 C5L3 architecture and data prep overview}
% \begin{verbatim}
% # baseline model(C5L3) architecture:  
%   net = nn.Sequential(
%       nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),
%       nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),
%       nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),
%       nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
%       nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),
%       nn.Flatten(),

%       nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),
%       nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),
%       nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),
%       nn.Linear(128, 10),
%   )
% # Params count: 3*128*3*3+128*256*3*3+256*512*3*2*3+512**2*3*3+256*16*512+512*256+256*128+1280=7.28M

% # Using default data transformation in example code-A
%   transformation = dict()
%   for data_type in ("train", "test"):
%     is_train = data_type=="train"
%     transformation[data_type] = tv_transforms.Compose((
%       [
%       # default data augmenation
%         tv_transforms.RandomRotation(degrees=15),
%         tv_transforms.RandomHorizontalFlip(),
%         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
      
%       # extra data augmentation:
%         tv_transforms.ColorJitter(
%           brightness=0.2, 
%           contrast=0.2,
%           saturation=0.2,
%           hue=0.1)
%       ] if is_train else []) + 
%     [
%         tv_transforms.ToTensor(),
%         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
%     ])
%   \end{verbatim}

\begin{lstlisting}[language=python]
  # baseline model(C5L3) architecture:  
  net = nn.Sequential(
    # conv layer: 5
      nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),
      nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),
      nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),
      nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
      nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),
      nn.Flatten(),

    # linear layer: 3, excluding the last layer
      nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),
      nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),
      nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),
      nn.Linear(128, 10),
  )
# ParamsCount: 3*128*3*3+128*256*3*3+256*512*3*2*3+512**2*3*3+256*16*512+512*256+256*128+1280=7.28M

# Using default data transformation in example code-A
  transformation = dict()
  for data_type in ("train", "test"):
    is_train = data_type=="train"
    transformation[data_type] = tv_transforms.Compose((
      [
      # default data augmenation
        tv_transforms.RandomRotation(degrees=15),
        tv_transforms.RandomHorizontalFlip(),
        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
      
      # extra data augmentation:
        tv_transforms.ColorJitter(
          brightness=0.2, 
          contrast=0.2,
          saturation=0.2,
          hue=0.1)
      ] if is_train else []) + 
    [
        tv_transforms.ToTensor(),
        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ])
\end{lstlisting}





\noindent Example of data augmentation on CIFAR-10:

% \begin{center}
%   \includegraphics[width=0.45\textwidth]{fig/original.png} % 设定图片宽度为文本宽度的80%
%   \includegraphics[width=0.45\textwidth]{fig/augmented.png} % 设定图片宽度为文本宽度的80%
% \end{center}
% \begin{center}
%   \includegraphics[width=0.8\textwidth]{fig/augmented.png} % 设定图片宽度为文本宽度的80%
% \end{center}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{fig/original.png} % 设定图片宽度
  \includegraphics[width=0.45\textwidth]{fig/augmented.png} % 设定图片宽度
  \caption{original(left), augmented(right):random filp/rotate/affine/colorjit } % 图片标题
  \label{fig:original} % 图片标签，方便引用
\end{figure}

\noindent\textbf{Training strategies}
We have 6 (basline models with different settings on CIFAR-10) + 3 (advanced network on CIFAR-10) + 1 (best model on MNIST) = 10 models in total to train. We decided to run 10 separates .ipynb on kaggle, 1 for each model with it's corresponding settings, insteading of packing all in one file for the following reasons:

\begin{itemize}
  \item Parallel: kaggle allows running 2 notebook at the same time, save time.
  \item Flexibility: we can adjust each model at any time.
  \item Fault tolerance: If we run everything in 1 file, we will loose everything if 1 error pop up, wasting time quota of free gpu usage.
\end{itemize}



\noindent\textbf{2.2 Baseline model + additional modifications beyond the Baseline(C5L3):}
\begin{itemize}
  \item Extra Data Augmentation: Aside from default techniques in baselline model including random flipping, random rotation, random affine transformation, we add random brightness adjustments, and saturation modifications.
  \item Optimizer Variant: Replaced Adam with SGD (learning rate=1e-3).
  \item Architectural Varaint: Removed dropout layers after convolutional layers (retained dropout for linear layers), or adding BatchNorm layers after convolutional layers.
\end{itemize}


% ========= describe exp setting =========
\noindent Results on \textcolor{red}{CIFAR-10} are summarized in the folloing table

\begin{table}[htbp]
  \centering
  \caption{ModelSettings and Performance}
  \label{tab:model-compare}
  \begin{tabular}{|c|c|ccccc|} 
    \hline
    Model &\textcolor{red}{C5L3 base} &  &  & C5L3 variant & &   \\
    \hline
    SettingNo. &1 &2  &\textcolor{blue}{3}  & 4 &5 &6   \\
    \hline
    Adam &  \textcolor{red}{ $\surd$ }   &  & \textcolor{blue}{$\surd$} & $\surd$ & $\surd$  & $\surd$\\ 
    SGD & & $\surd$   & & &  &\\
    ExtraDataAug& &  & \textcolor{blue}{$\surd$} &  $\surd$ &  &$\surd$\\
    dropout &\textcolor{red}{all} & all   & \textcolor{blue}{linear}& none & linear  & linear \\ 
    BatchNorm & & & \textcolor{blue}{conv} & conv  & conv & conv \\ 
    MaxEpoch & \textcolor{red}{150} & 150 & \textcolor{blue}{150} & 150  & 150 & 256 \\
    \hline
    BestTestAccu & \textcolor{red}{85.11} &84.95 &\textcolor{blue}{89.31} & 89.69& 88.34 & 89.73 \\
    \hline
  \end{tabular} 
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{fig/train_result_1.png} % 设定图片宽度
  \includegraphics[width=0.45\textwidth]{fig/train_result_2.png} % 设定图片宽度
  \caption{model settings 1(left) 2(right)} % 图片标题
  \label{fig:curve} % 图片标签，方便引用
\end{figure}


\textcolor{red}{\# Note:} bacthsize = 64, Epoch num = 150, dropout(conv) indicate whether we use dropout layer in convlution layer.

\noindent - Results Interpretation: 
\begin{itemize}
  \item BatchNorm considerably improve accuracy (+4.5\%), compared with other techniques.
  \item choosing SGD or Adam will not significantly affect accuracy, they just converge to almost the same accuracy at different speeds (in terms of epochs). \textcolor{blue}{attach figure of loss curve}
  \item dropout layers have almost no effect to the performance of model, since we have tested 2 scenarios: 1) keep dropout layers only in Linear layer (popular method) 2) removing all dropout layers; and they do not differ much in terms of test accu.
  \item Notice that the last column of the table, we re-do the 3rd settings for 256 epochs, and the test accuracy only improve for a margin. 
\end{itemize}

% ============== use advanced model structure: vgg16/resnet18/resnet50 ================
\subs{2.3 Deeper Neural Network}
Although our basline model can achieve around 90\% accuracy on test set, deeper neural network can always (at a high prob) handle complicated features better, here, cifar-10 images have complex features. So we decided to try deeper networks \texttt{vgg16, ResNet18, ResNet50}.

\texttt{vgg16} stacking 5 blocks of conv->batchnorm->relu->conv->batchnorm->relu->maxpool, hoping to extract more abstract representation of features.

\texttt{ResNet18} stacking 4 basic ResidualBlocks, by doing so it allows gradient to propagate without exploding or disappearing.

\texttt{ResNet50} stack 4 special ResidualBlock (named BottleNeck Block) and allows gradient to propagate in even deeper network.

\begin{table}[htbp]
  \centering
  \caption{ModelSettings and Performance}
  \label{tab:model-compare}
  \begin{tabular}{|c|ccc|} 
    \hline
    Model & vgg16 & ResNet18 & ResNet50  \\
    \hline
    TestAcc & * & * & *  \\
    \hline
  \end{tabular} 
\end{table}






\subs{2.4 MNIST dataset}
We first use basline model together with our best params on MNIST dataset. Check notebook codes.


\subs{Interesting findings and reflections}
1) visualize data after data augmentation
2) what have i learnt.
3) \textcolor{blue}{due to curiosity，可以用batchsize 32以及256 epoch对best model进行验证，看看accu能否上90}

\newpage
\secs{B. Build a Neural Network From Scratch}
\subs{1. AutoGrad: To Learn from the better}
When attempting to build a deep learning framework from scratch, I referenced numerous materials, including Example-Code-Part-B and PyTorch. However, PyTorch is an immensely large framework that integrates numerous heavily optimized modules. The implementation logic of most modules has become unrecognizable under massive speed and compatibility optimizations, which makes it akin to a "rocket science project" for a beginner. At least for me, it is not a good reference material.\\

In Example-Code-Part-B, the implementation of backpropagation relies on MANUALLY deriving gradients and writing them into .backward() methods, which are then called in a nested way to perform backward propagation when gradients are needed. This raised concerns for me: For more complex network architectures (e.g., ResidualBlock in ResNet), manual gradient derivation would be nearly impossible. Does this mean that after painstakingly "implementing" the simple functionalities required for the assignment, I still cannot gain much insight into the model DL framework (especially AutoGrad and Computing Graph), or to use these components to build more sophisticated networks?\\

This concern drove me to explore further, leading me to discover the Chainer framework. Its core innovation is automatic differentiation via the dynamic computational graph (Define-by-Run), which allows users to construct the computation graph on-the-fly during the forward propagation process then attain gradients automatically, rather than predefining a static structure. Chainer was the first framework to propose dynamic computational graphs and inspired PyTorch's design. However, Chainer's high complexity still made it unsuitable for a novice learner.\\

I then discovered the book Building Deep Learning Frame, which introduces a framework called DeZero—a simplified version of Chainer. It is sufficiently complex and flexible, yet not overly daunting for learners. The book provides the complete source code for DeZero, but simply copying the code would not have benefited me. Since I started this project early (immediately after the assignment was released), I spent two full weeks studying every line of DeZero's code character by character. I abandoned the author’s original implementation of the Conv2d module and reimplemented it from scratch, and I independently implemented FocalLoss module, which were not provided by the author, as tests to verify my understanding. All of this was made possible by the concepts of automatic differentiation and dynamic computational graphs, freeing me from the agony of manual gradient derivation and debugging, and preparing me to build complex deep networks like ResNet18/50 in the future.

\subs{2. File Structure of the framework}
\textbf{The core of my framework is Define-by-Run AutoGrad, it builds connections between variables for later back-propagation when doing calculation.} The DeZero module is composed of a series of \texttt{.py} files, which can be imported like a standard Python package using \texttt{import}. Below I will outline the module's file structures and map the assignment-required functionalities to their corresponding implementations position. For certain noteworthy implementation approaches, I will briefly discuss them here.

I want to emphasize again: I did not simply copy and submit the DeZero module as-is. The annotations and drafts within the module serve as evidence of this—I invested significant time and effort, ultimately not to deceive myself, but to deeply understand and internalize the concepts.

\begin{verbatim}
  dezero-master: .
  │  core.py --------# base class: Function and Variable(a torch.tensor like data structure)
  │  core_copy_beforeCuPY.py --------# ignore 
  │  core_simple.py --------# ignore
  │  cuda.py
  │  dataloaders.py
  │  datasets.py
  │  functions.py --------# Sigmoid/LeakyReLU/Dropout/FocalLoss/
  │  functions_conv.py --------#conv modules: F.Conv2dV function
  │  layers.py --------# Linear/Conv2dV/BatchNorm2d/
  │  models.py --------# C5L3/vgg16/ResNet18/50
  │  optimizers.py --------# SGD/Adam
  │  utils.py
  │  __init__.py
\end{verbatim}

\begin{itemize}
  \item[a)] Sigmoid: ../dezero-master/dezero/functions.py - line 457
  \item[b)] LeakyReLU: ../dezero-master/dezero/functions.py - line 509
  \item[c)] SELU: not implemented yet
  \item[d)] Linear: ../dezero-master/dezero/layers.py - line 118
  \item[e)] Conv2dV: ../dezero-master/dezero/layers.py - line 221, the original conv2d method is complicated, i wrote a new one (named Conv2dV) with im2col in forward and col2im in backward, almost as fast as the original one.
  \item[f)] Dropout: ../dezero-master/dezero/functions.py - line 691
  \item[g)] BatchNorm2d: ../dezero-master/dezero/layers.py - line 311, automatically detect num\_features, so no need to add this config
  \item[h)] SigmoidFocalLoss: ../dezero-master/dezero/functions.py - Line 570
  \item[i)] SGD: ../dezero-master/dezero/optimizers.py - line 38, 2 ways of implementation, 1)manual: forward+backward, marginally faster 2)auto: simply define the forward caculation, and get gradients through auto grad, almost as fast as 1).
  \item[j)] Adam: ../dezero-master/dezero/optimizers.py - line 84   
\end{itemize}


做完1的试验后获取best 参数，接着用dezero堆叠一个相同结构。



\secs{Reflections}
\noindent\textbf{From BackwardPropagation to AutoGrad}: In example code part-B we can learn how forward pass and backward pass work together in a neural network, but implementing backward pass is complicated and sometimes impossible (especially in complicated computation). Meanwhile we know that all computation in deep learning are basically consist of few basic functions like exponents, add, subtract, multiplication etc, so we can utilizie chain rule to calculate gradient no matter how complicated a computation might be. This is how AutoGrad comes into play, and this is the core of  all deep learning frameworks, including pytorch.\\

\noindent\textbf{Understand by creating}: During the process of building a framework, i learn extensively and experience moments of revelation like "ah, this is how a neural network works!" or "so this algorithm can be implemented this way!" These insights are unattainable through merely using existing tools like pytorch. Some concepts can only be understood by creating, some truths can only be seen by building. 

For instance, some may view deep learning frameworks as mere libraries pieced together with layers and functions. In reality, frameworks go far beyond that. A framework can be considered a programming language—specifically, one with automatic differentiation capabilities (recently termed a "differentiable programming language").
















% \secs{A4.1 (Implementing the Gradient Method)}
% We want to minimize the objective function
% \[
% \min_{x\in\mathbb{R}^2} f(x) = \frac{1}{2}x_1^4-x_1^3-x_1^2+x_1^2x_2^2+\frac{1}{2}x_2^4-x_2^2
% \]
% by gradient descent methods with different initial points and stepsize strategies, as presented in the following.\\

% \noindent Initial points: 
% \[
% \chi^0 := \left\{
% \begin{bmatrix}
% -\frac{1}{2} \\ 1
% \end{bmatrix},
% \begin{bmatrix}
% -\frac{1}{2} \\ \frac{1}{2}
% \end{bmatrix},
% \begin{bmatrix}
% -\frac{1}{4} \\ -\frac{1}{2}
% \end{bmatrix},
% \begin{bmatrix}
% \frac{1}{2} \\ -\frac{1}{2}
% \end{bmatrix},
% \begin{bmatrix}
% \frac{1}{2} \\ 1
% \end{bmatrix}
% \right\}
% \]
% \noindent stationary points of $f(x)$:
% \[
% \chi^* := \left\{
% \begin{bmatrix}
% 0 \\ 0
% \end{bmatrix},
% \begin{bmatrix}
% 2 \\ 0
% \end{bmatrix},
% \begin{bmatrix}
% -\frac{1}{2} \\ 0
% \end{bmatrix},
% \begin{bmatrix}
% 0 \\ 1
% \end{bmatrix},
% \begin{bmatrix}
% 0 \\ -1
% \end{bmatrix}
% \right\}
% \]



% %------------ backtracking--------------
% \subsection*{1. Backtracking}
% % \begingroup
% % \setlength{\intextsep}{0pt}%
% % \setlength{\columnsep}{0pt}%
% % \begin{wrapfigure}{r}{0.65\textwidth}%靠文字内容的右侧
% %   \centering
% %   \includegraphics[width=\linewidth]{Back Tracking.png}
% %   \caption{Backtracking Line Search}
% % \end{wrapfigure}

% Back Tracking line search: choose the largest $\alpha_k \in \{\sigma^k: k = 0,1,...\}$ that satisfies Armijo condition $f(x_k+\alpha_k d_k)-f(x_k)\leq \gamma \alpha_k \nabla f(x_k)^T d_k$ with $(\sigma, \gamma) = (0.5,0.1)$.\\
% \\
% Performance (in terms of iteration) of Backtracking Line Search stepsize strategy are shown below:\\
% \\
%   \begin{tabular}{ccc}
    
%     \hline
%     $x_0$ & iteration & limit point $x^*$\\
%     \hline
%           (-0.50,1.00) & 13 & (2.00,-0.00)  \\ % backtrack
%           (-0.50,0.50) & 325 & (-0.00,1.00) \\ % backtrack
%           (-0.25,-0.50) & 467 & (-0.00,-1.00)  \\ % backtrack
%           (0.50,-0.50) & 12 & (2.00,0.00)  \\ % backtrack
%           (0.50,1.00) & 10 & (2.00,-0.00)  \\ % backtrack
%           \hline
%           \multicolumn{3}{c}{Backtracking Line Search}\\
%           \hline
%   \end{tabular}

% %   \endgroup
% \newpage
% %------------------ exact line search ----------------------
% \subs{2. Exact Line Search}

% % \begingroup
% % \setlength{\intextsep}{0pt}%
% % \setlength{\columnsep}{0pt}%
% % \begin{wrapfigure}{r}{0.65\textwidth}%靠文字内容的右侧
% %   \centering
% %   \includegraphics[width=\linewidth]{Exact Line Search.png}
% %   %\caption{Exact Line Search}
% % \end{wrapfigure}

% Exact Line Search: aim at choosing the stepsize $\alpha_k$ that 
% \[
% \alpha_k = \underset{\alpha_k \geq 0}{argmin}\; f(x_k+\alpha_k d_k)
% \]
% Performance of Exact line search stepsize strategy are shown below:\\

% \noindent\begin{tabular}{ccc}
    
%   \hline
%   $x_0$ & iteration & limit point $x^*$\\
%   \hline
%   (-0.50,1.00) & 295 & (-0.00,1.00)   \\ % exactlineS
%   (-0.50,0.50) & 296 & (-0.00,1.00)  \\ % exactlineS
%   (-0.25,-0.50) & 375 & (-0.00,-1.00)  \\ % exactlineS
%   (0.50,-0.50) & 9 & (2.00,0.00) \\ % exactlineS
%   (0.50,1.00) & 6 & (2.00,0.00)  \\ % exactlineS
%         \hline
%         \multicolumn{3}{c}{Exact Line Search}\\
%         \hline
% \end{tabular}\\
% \\
% \textbf{\# PS}: the paths of 2 consecutive steps are not perpendicular because we constraint $\alpha_k \leq 1$ (because we search $\alpha$ in $[0,a]$, where $a=1$) insetead of not setting any constraint to them.

% % \endgroup
% \vspace{1cm}

% %------------- diminishing -----------------
% \subs{3. Diminishing Stepsize}

% % \begingroup
% % \setlength{\intextsep}{0pt}%
% % \setlength{\columnsep}{0pt}%
% % \begin{wrapfigure}{r}{0.65\textwidth}%靠文字内容的右侧
% %   \centering
% %   \includegraphics[width=\linewidth]{Diminishing Stepsize.png}
% %   %\caption{Diminishing Stepsize}
% % \end{wrapfigure}

% \noindent Diminishing Stepsize: we simply set
% \[
%   \alpha_k = \frac{1}{\sqrt{k+2}}
% \]
% where $k$ is the round of iteration.\\
% Performance of diminishing stepsize strategy are shown below:

% \noindent\begin{tabular}{ccc}
    
%   \hline
%   $x_0$ & iteration & limit point $x^*$\\
%   \hline
%   (-0.50,1.00) & 47 & (2.00,0.00)  \\ % diminishing
%   (-0.50,0.50) & 8523 & (-0.00,1.00) \\ % diminishing
%   (-0.25,-0.50) & 8501 & (-0.00,-1.00)  \\ % diminishing
%   (0.50,-0.50) & 47 & (2.00,-0.00)  \\ % diminishing
%   (0.50,1.00) & 47 & (2.00,0.00)  \\ % diminishing
%         \hline
%         \multicolumn{3}{c}{Diminishing Stepsize}\\
%         \hline
% \end{tabular}\\
% \\
% \textbf{\# PS}: Since HW sheet has no requirement on $k$, we follow the common sense that $k$ starts from 1, so the first stepsize is
% \[
%   \alpha_1 = \frac{1}{\sqrt{1+2}} = \frac{1}{\sqrt{3}} 
% \]

% % \endgroup

% \newpage
% %------------- conclusion of stepsize strategy ---------------
% Then we can conclude the performance of different stepsize strategies as the following table:

  % \begin{center}
  %   \begin{tabular}{|c|cccc|} 
  %       \hline
  %       Methods & $x_0$ & iteration & limit point $x^*$ & Global Minimum?\\
  %       \hline
  %       &(-0.50,1.00) & 13 & (2.00,-0.00) & yes\\ % backtrack
  %       &(-0.50,0.50) & 325 & (-0.00,1.00) & no \\ % backtrack
  %       Back Tracking&(-0.25,-0.50) & 467 & (-0.00,-1.00) & no \\ % backtrack
  %       &(0.50,-0.50) & 12 & (2.00,0.00) & yes \\ % backtrack
  %       &(0.50,1.00) & 10 & (2.00,-0.00) & yes \\ % backtrack
  %       \hline
      %   \hline
      %   &(-0.50,1.00) & 295 & (-0.00,1.00) & no \\ % exactlineS
      %   &(-0.50,0.50) & 296 & (-0.00,1.00) & no \\ % exactlineS
      %   Exact Line Search&(-0.25,-0.50) & 375 & (-0.00,-1.00) & no \\ % exactlineS
      %   &(0.50,-0.50) & 9 & (2.00,0.00) & yes \\ % exactlineS
      %   &(0.50,1.00) & 6 & (2.00,0.00) & yes \\ % exactlineS
      %   \hline
      %   \hline
      %   &(-0.50,1.00) & 47 & (2.00,0.00) & yes\\ % diminishing
      %   &(-0.50,0.50) & 8523 & (-0.00,1.00) & no \\ % diminishing
      %   Diminishing Stepsize&(-0.25,-0.50) & 8501 & (-0.00,-1.00) & no \\ % diminishing
      %   &(0.50,-0.50) & 47 & (2.00,-0.00) & yes\\ % diminishing
      %   &(0.50,1.00) & 47 & (2.00,0.00) & yes \\ % diminishing
      % \hline
%     \end{tabular} 
% \end{center}
% \newpage

% % \secs{A4.2 (Inertial Gradient Method)}

% % \noindent The convergence trace of gradient method with momentum of $\beta \in \{0.3,0.5,0.7,0.9\}$ and different initial points are shown below respectively:\\
% % \vspace{5mm}

% % \noindent\includegraphics[width=0.5\linewidth]{GD with momentum, beta=03.png}
% % \includegraphics[width=0.5\linewidth]{GD with momentum, beta=05.png}\\
% % \includegraphics[width=0.5\linewidth]{GD with momentum, beta=07.png}
% % \includegraphics[width=0.5\linewidth]{GD with momentum, beta=09.png}\\

% \subs{Performance Analysis}
% by comparing the average iteration numbers of GD with momentum and GD with different stepsize strategies (discussed in part A4.1):
% \begin{center}
%   \begin{tabular}{c|ccc|cccc}
%     \hline
%     \multicolumn{1}{c}{} & \multicolumn{3}{c}{Stepsize strategies} & \multicolumn{4}{c}{GD with momentum}\\
%     \hline
%     &Backtrack & Exact LineSearch & Diminishing & $\beta = 0.3$ & $\beta = 0.5$ & $\beta = 0.7$ & $\beta = 0.9$\\
%     \hline
%     Average Iteration & 164.5 & 196.2 & 3433.0 & 104.8& 51.8 & 90.2 & 1250.0\\
%     \hline
%     Probablity to Global Min & 0.6 & 0.4 & 0.6 & 0.8 & 1.0 & 1.0 &0.8\\
%     \hline
%   \end{tabular}
% \end{center}
% It is easy to see that \textbf{the average iteration (averaging across 5 different initial points) it takes to converge to some limit point is noticeably larger using stepsize strategies than GD with momentum in general.}  \\
% \\
% And we also notice GD with momentum converger faster when $\beta$ change from $0.3$ to $0.5$, then converger slower when $\beta$ goes from $0.5$ to $0.9$, how to interpretate this phenomenon? we can first look at the detail of how $\beta$ affect convergence: 

% \begin{center}
%   \begin{tabular}{|c|cccc|} 
%       \hline
%       $\beta$ & $x_0$ & iteration & limit point $x^*$ & Global Minimum? \\
%       \hline
%       &(-0.50,1.00) & 26 & (2.00,0.00) & yes\\ % momentum beta 0.3
%       &(-0.50,0.50) & 33 & (2.00,-0.00) & yes \\ % momentum beta 0.3
%       $\beta = 0.3$&\textcolor{red}{(-0.25,-0.50)} & \textcolor{red}{417} & \textcolor{red}{(-0.00,-1.00)} & no \\ % momentum beta 0.3
%       &(0.50,-0.50) & 24 & (2.00,0.00) & yes \\ % momentum beta 0.3
%       &(0.50,1.00) & 24 & (2.00,0.00) & yes \\ % momentum beta 0.3
%       \hline
%       \hline
%       &(-0.50,1.00) & 39 & (2.00,0.00) & yes \\ % momentum beta 0.5
%       &(-0.50,0.50) & 56 & (2.00,0.00) & yes \\ % momentum beta 0.5
%       $\beta = 0.5$&(-0.25,-0.50) & 88 & (2.00,0.00) & yes \\ % momentum beta 0.5
%       &(0.50,-0.50) & 37 & (2.00,-0.00) & yes \\ % momentum beta 0.5
%       &(0.50,1.00) & 39 & (2.00,0.00) & yes \\ % momentum beta 0.5
%       \hline
%       \hline
%       &(-0.50,1.00) & 105 & (2.00,0.00) & yes \\ % momentum beta 0.7
%       &(-0.50,0.50) & 88 & (2.00,0.00) & yes \\ % momentum beta 0.7
%       $\beta = 0.7$&(-0.25,-0.50) & 102 & (2.00,-0.00) & yes \\ % momentum beta 0.7
%       &(0.50,-0.50) & 77 & (2.00,-0.00) & yes \\ % momentum beta 0.7
%       &(0.50,1.00) & 79 & (2.00,-0.00) & yes \\ % momentum beta 0.7
%     \hline
%     \hline
%     &(-0.50,1.00) & 268 & (2.00,0.00) & yes \\ % momentum beta 0.9
%     &(-0.50,0.50)& 258 & (2.00,0.00) & yes \\ % momentum beta 0.9
%     $\beta = 0.9$&(-0.25,-0.50) & 276 & (2.00,0.00) & yes \\ % momentum beta 0.9
%     &(0.50,-0.50) & 297 & (2.00,-0.00) & yes \\ % momentum beta 0.9
%     &\textcolor{red}{(0.50,1.00)} & \textcolor{red}{5151} & \textcolor{red}{(-0.00,1.00)} & no \\ % momentum beta 0.9
%     \hline
%   \end{tabular} 
% \end{center}
% Notice when $\beta = 0.3$ and starting from the initial point $(-0.25,-0.5)$, GD converge to $(0,-1)$ while other initial points all converge to $(2,0)$, this is because the first several stepsize of GD heppen to be too large from this initial point and \textbf{pushing the trace to an area not ideal for fast convergence}, and by coincidence the trace finally converge to a different limit point from the other initial points, and the iterations it takes to converge go very high, this should \textbf{be treated as an anomoly.} Same with $\beta = 0.9$ initial point $(0.5,1.0)$: this very initial condition just happen to be not ideal for fast convergence.\\
% After dropping this anomoly we can get an averge iteration number of $26.75$ for $\beta = 0.3$, and the trend becomes obvious: $\beta$ smaller, converge faster. \\
% \\
% Another observation is that \textbf{GD with momentum is more likely to converge to the global minimum} $(2,0)$ than GD with stepsize strategies, this is also easy to explain: with "momentum" (brought by the momentum term $\beta (x_k-k_{k-1})$), the trace is more likely to "escape" from the local minimum.
% \newpage





\end{document}


